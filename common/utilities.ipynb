{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Utilities Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def data_splitter(df, train_size, stratification_column, seed=42):\n",
    "    \"\"\"\n",
    "    Function splits dataframe into 2 parts. \n",
    "    \"\"\"\n",
    "    \n",
    "    if stratification_column is not None:\n",
    "        # stratified split\n",
    "        df_train, df_test = train_test_split(df, train_size=train_size, random_state=seed, stratify=df[stratification_column])\n",
    "    else:\n",
    "        df_train, df_test = train_test_split(df, train_size=train_size, random_state=seed)\n",
    "    \n",
    "    df_train, df_test = df_train.reset_index(drop=True), df_test.reset_index(drop=True)\n",
    "    \n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_identifiers(df, dtype='float'):\n",
    "    \"\"\"\n",
    "    Find identifier columns in entire dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe which is combination of train-validation-test\n",
    "    dtype : the data type identifiers cannot have, 'float' by default\n",
    "            i.e., if a feature has this data type, it cannot be an identifier\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    The dataframe of identifiers\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the dataframe of identifiers\n",
    "    df_id = df[[var for var in df.columns\n",
    "                # If the data type is not dtype\n",
    "                if (df[var].dtype != dtype\n",
    "                    # If the value is unique for each sample\n",
    "                    and df[var].nunique(dropna=True) == df[var].notnull().sum())]]\n",
    "    \n",
    "    return df_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unique_identifiers(df, df_id):\n",
    "    df = df.drop(columns=np.intersect1d(df_id.columns, df.columns))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_checker(df):\n",
    "    \"\"\"\n",
    "    The NaN checker\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : the dataframe(train+test+validation) which contains NaNs. You should replace all other\n",
    "    representation of NaN like nan, n/a with np.NaN\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    The dataframe of variables with NaN, their proportion of NaN and data type\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the dataframe of variables with NaN, their proportion of NaN and data type\n",
    "    df_nan = pd.DataFrame([[var, df[var].isna().sum() / df.shape[0], df[var].dtype]\n",
    "                           for var in df.columns if df[var].isna().sum() > 0],\n",
    "                          columns=['var', 'proportion', 'dtype'])\n",
    "    \n",
    "    # Sort df_nan in accending order of the proportion of NaN\n",
    "    df_nan = df_nan.sort_values(by='proportion', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return df_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def impute_data(columns, df_train, df_val, df_test, strategy):\n",
    "    \n",
    "    si = SimpleImputer(missing_values=np.nan, strategy=strategy)\n",
    "    \n",
    "    # find statistics based on train data\n",
    "    df_train[columns] = si.fit_transform(df_train[columns])\n",
    "    \n",
    "    # impute validation data using train statistics\n",
    "    df_val[columns] = si.transform(df_val[columns])\n",
    "    \n",
    "    # impute test data using train statistics\n",
    "    df_test[columns] = si.transform(df_test[columns])\n",
    "    \n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://imbalanced-learn.org/dev/references/generated/imblearn.over_sampling.SMOTEN.html\n",
    "from imblearn.over_sampling import SMOTEN\n",
    "\n",
    "def perform_smote(X_train, y_train, strategy):\n",
    "    # The SMOTE\n",
    "    smote = SMOTEN(random_state=42, sampling_strategy=strategy)\n",
    "\n",
    "    # Augment the training data\n",
    "    X_smote_train, y_smote_train = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    return X_smote_train, y_smote_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Tools:\n",
    "\n",
    "- F2 Score\n",
    "- Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F2 Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "def f2_measure(y_true, y_pred):\n",
    "    return fbeta_score(y_true, y_pred, beta=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def confusion_matrix_plotter(y_true, y_pred, title):\n",
    "    # compute confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # plot confusion matrix\n",
    "    fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "    ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        for j in range(conf_matrix.shape[1]):\n",
    "            ax.text(x=j, y=i, s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
    "\n",
    "    plt.xlabel('Predictions', fontsize=18)\n",
    "    plt.ylabel('Actuals', fontsize=18)\n",
    "    plt.title(title, fontsize=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modeling Utils:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import PredefinedSplit\n",
    "\n",
    "def get_train_val_ps(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Get the:\n",
    "    feature matrix and target velctor in the combined training and validation data\n",
    "    target vector in the combined training and validation data\n",
    "    PredefinedSplit\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : the feature matrix in the training data\n",
    "    y_train : the target vector in the training data\n",
    "    X_val : the feature matrix in the validation data\n",
    "    y_val : the target vector in the validation data  \n",
    "\n",
    "    Return\n",
    "    ----------\n",
    "    The feature matrix in the combined training and validation data\n",
    "    The target vector in the combined training and validation data\n",
    "    PredefinedSplit\n",
    "    \"\"\"  \n",
    "\n",
    "    # Combine the feature matrix in the training and validation data\n",
    "    X_train_val = np.vstack((X_train, X_val))\n",
    "\n",
    "    # Combine the target vector in the training and validation data\n",
    "    y_train_val = np.vstack((y_train.reshape(-1, 1), y_val.reshape(-1, 1))).reshape(-1)\n",
    "\n",
    "    # Get the indices of training and validation data\n",
    "    train_val_idxs = np.append(np.full(X_train.shape[0], -1), np.full(X_val.shape[0], 0))\n",
    "\n",
    "    # The PredefinedSplit\n",
    "    ps = PredefinedSplit(train_val_idxs)\n",
    "\n",
    "    return X_train_val, y_train_val, ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pickling Utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_object(pkl_file_path, pkl_file_name, file_to_save):\n",
    "    with open(pkl_file_path+\"/\"+pkl_file_name+\".pkl\", 'wb') as file:\n",
    "        pickle.dump(file_to_save, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_object(pkl_file_path, pkl_file_name):\n",
    "    \n",
    "    with open(pkl_file_path+\"/\"+pkl_file_name+\".pkl\", 'rb') as file:\n",
    "        pickled_object = pickle.load(file)\n",
    "    \n",
    "    return pickled_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "- [Utilities](https://github.com/yuxiaohuang/teaching/blob/master/gwu/machine_learning_I/fall_2020/code/utilities/p2_shallow_learning/pmlm_utilities_shallow.ipynb)\n",
    "- [Regression](https://github.com/yuxiaohuang/teaching/blob/master/gwu/machine_learning_I/fall_2020/code/p2_shallow_learning/p2_c1_data_preprocessing/code_example/regression.ipynb)\n",
    "- [Classification](https://github.com/yuxiaohuang/teaching/blob/master/gwu/machine_learning_I/fall_2020/code/p2_shallow_learning/p2_c1_data_preprocessing/code_example/classification.ipynb)\n",
    "- [Tree Models](https://github.com/yuxiaohuang/teaching/blob/master/gwu/machine_learning_I/fall_2020/code/p2_shallow_learning/p2_c2_supervised_learning/p2_c2_s5_tree_based_models/code_example/code_example.ipynb)\n",
    "- [Logistic Regression](https://github.com/yuxiaohuang/teaching/blob/master/gwu/machine_learning_I/fall_2020/code/p2_shallow_learning/p2_c2_supervised_learning/p2_c2_s3_logistic_regression/case_study/case_study_bcw.ipynb)\n",
    "- [Imbalanced Classification German Bank](https://machinelearningmastery.com/imbalanced-classification-of-good-and-bad-credit/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
