{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "- [Utilities](https://github.com/yuxiaohuang/teaching/blob/master/gwu/machine_learning_I/fall_2020/code/utilities/p2_shallow_learning/pmlm_utilities_shallow.ipynb)\n",
    "- [Regression](https://github.com/yuxiaohuang/teaching/blob/master/gwu/machine_learning_I/fall_2020/code/p2_shallow_learning/p2_c1_data_preprocessing/code_example/regression.ipynb)\n",
    "- [Classification](https://github.com/yuxiaohuang/teaching/blob/master/gwu/machine_learning_I/fall_2020/code/p2_shallow_learning/p2_c1_data_preprocessing/code_example/classification.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_splitter(df, train_size, stratification_column, seed=42):\n",
    "    \"\"\"\n",
    "    Function splits dataframe into 2 parts. \n",
    "    \"\"\"\n",
    "    \n",
    "    if stratification_column is not None:\n",
    "        # stratified split\n",
    "        df_train, df_test = train_test_split(df, train_size=train_size, random_state=seed, stratify=df[stratification_column])\n",
    "    else:\n",
    "        df_train, df_test = train_test_split(df, train_size=train_size, random_state=seed)\n",
    "    \n",
    "    df_train, df_test = df_train.reset_index(drop=True), df_test.reset_index(drop=True)\n",
    "    \n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split_view_distribution(df_train, df_val, df_test, target_column):\n",
    "    # TODO this will take a train-test-validation dataframes and plot the distribution of target to show stratified subplots\n",
    "    # if discrete, then plot barcharts\n",
    "    # if continuous, then plot histogram\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_var_checker(df_train, df_val, df_test, target):\n",
    "    \"\"\"\n",
    "    The common variables checker\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_train : the dataframe of training data\n",
    "    df_val : the dataframe of validation data\n",
    "    df_test : the dataframe of test data\n",
    "    target : the name of the target\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    The dataframe of common variables between the training, validation and test data\n",
    "    {\"common_var\":[....]}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the dataframe of common variables between the training, validation and test data\n",
    "    df_common_var = pd.DataFrame(np.intersect1d(np.intersect1d(df_train.columns, df_val.columns), np.union1d(df_test.columns, [target])),\n",
    "                                 columns=['common_var'])\n",
    "                \n",
    "    return df_common_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_uncommon_features(df, df_common_var):\n",
    "    # find uncommon features\n",
    "    uncommon_features = np.setdiff1d(df.columns, df_common_var['common_var'])\n",
    "    \n",
    "    # print(uncommon_features)\n",
    "    \n",
    "    # delete the uncommon features\n",
    "    df = df.drop(columns=uncommon_features)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_identifiers(df, dtype='float'):\n",
    "    \"\"\"\n",
    "    Find identifier columns in entire dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe which is combination of train-validation-test\n",
    "    dtype : the data type identifiers cannot have, 'float' by default\n",
    "            i.e., if a feature has this data type, it cannot be an identifier\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    The dataframe of identifiers\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the dataframe of identifiers\n",
    "    df_id = df[[var for var in df.columns\n",
    "                # If the data type is not dtype\n",
    "                if (df[var].dtype != dtype\n",
    "                    # If the value is unique for each sample\n",
    "                    and df[var].nunique(dropna=True) == df[var].notnull().sum())]]\n",
    "    \n",
    "    return df_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unique_identifiers(df, df_id):\n",
    "    df = df.drop(columns=np.intersect1d(df_id.columns, df.columns))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_transformer(df, datetime_vars):\n",
    "    \"\"\"\n",
    "    The datetime transformer\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : the dataframe\n",
    "    datetime_vars : the datetime variables as list\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    The dataframe where datetime_vars are transformed into the following 6 datetime types:\n",
    "    year, month, day, hour, minute and second\n",
    "    \"\"\"\n",
    "    \n",
    "    # The dictionary with key as datetime type and value as datetime type operator\n",
    "    dict_ = {'year'   : lambda x : x.dt.year,\n",
    "             'month'  : lambda x : x.dt.month,\n",
    "             'day'    : lambda x : x.dt.day,\n",
    "             'hour'   : lambda x : x.dt.hour,\n",
    "             'minute' : lambda x : x.dt.minute,\n",
    "             'second' : lambda x : x.dt.second}\n",
    "    \n",
    "    # Make a copy of df\n",
    "    df_datetime = df.copy(deep=True)\n",
    "    \n",
    "    # For each variable in datetime_vars\n",
    "    for var in datetime_vars:\n",
    "        # Cast the variable to datetime\n",
    "        df_datetime[var] = pd.to_datetime(df_datetime[var])\n",
    "        \n",
    "        # For each item (datetime_type and datetime_type_operator) in dict_\n",
    "        for datetime_type, datetime_type_operator in dict_.items():\n",
    "            # Add a new variable to df_datetime where:\n",
    "            # the variable's name is var + '_' + datetime_type\n",
    "            # the variable's values are the ones obtained by datetime_type_operator\n",
    "            df_datetime[var + '_' + datetime_type] = datetime_type_operator(df_datetime[var])\n",
    "            \n",
    "    # Remove datetime_vars from df_datetime\n",
    "    df_datetime = df_datetime.drop(columns=datetime_vars)\n",
    "                \n",
    "    return df_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_checker(df):\n",
    "    \"\"\"\n",
    "    The NaN checker\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : the dataframe(train+test+validation) which contains NaNs. You should replace all other\n",
    "    representation of NaN like nan, n/a with np.NaN\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    The dataframe of variables with NaN, their proportion of NaN and data type\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the dataframe of variables with NaN, their proportion of NaN and data type\n",
    "    df_nan = pd.DataFrame([[var, df[var].isna().sum() / df.shape[0], df[var].dtype]\n",
    "                           for var in df.columns if df[var].isna().sum() > 0],\n",
    "                          columns=['var', 'proportion', 'dtype'])\n",
    "    \n",
    "    # Sort df_nan in accending order of the proportion of NaN\n",
    "    df_nan = df_nan.sort_values(by='proportion', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return df_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dfs(dfs:list):\n",
    "    \"\"\"List of dataframes to combine into 1 dataframe\"\"\"\n",
    "    df = pd.concat([dfs], sort=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_dfs(combined_df, train_len, val_len):\n",
    "    \"\"\"\n",
    "    combined_df assumes that dataframes were combined in following order: train, validation, test\n",
    "    \"\"\"\n",
    "    # Separating the training data\n",
    "    df_train = df.iloc[:train_len, :]\n",
    "\n",
    "    # Separating the validation data\n",
    "    df_val = df.iloc[train_len:(train_len + val_len), :]\n",
    "\n",
    "    # Separating the test data\n",
    "    df_test = df.iloc[(train_len + val_len):, :]\n",
    "    \n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missing_values(df_nan, df_train, df_val, df_test):\n",
    "    \"\"\"\n",
    "    df_nan is output of nan_checker(..) function which returns df that contains columns: var, proportion, dtype\n",
    "    \"\"\"\n",
    "    \n",
    "    # we want to remove columns which have data type of float64 and keep columns of data type string\n",
    "    df_miss = df_nan[df_nan['dtype'] == 'float64'].reset_index(drop=True)\n",
    "    \n",
    "    # print(df_miss)\n",
    "    \n",
    "    if len(df_miss['var']) > 0:\n",
    "        # Remove rows with missing values from df_train\n",
    "        df_remove_train = df_train.dropna(subset=np.intersect1d(df_miss['var'], df_train.columns),\n",
    "                                        inplace=False)\n",
    "\n",
    "        # Remove rows with missing values from df_val\n",
    "        df_remove_val = df_val.dropna(subset=np.intersect1d(df_miss['var'], df_val.columns),\n",
    "                                        inplace=False)\n",
    "\n",
    "        # Remove rows with missing values from df_test\n",
    "        df_remove_test = df_test.dropna(subset=np.intersect1d(df_miss['var'], df_test.columns),\n",
    "                                    inplace=False)\n",
    "        \n",
    "        return df_remove_train, df_remove_val, df_remove_test\n",
    "    \n",
    "    else:\n",
    "        # no rows contain missing data thus return nothing\n",
    "        return None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_values(df_nan, df_train, df_val, df_test, strategy=\"mean\"):\n",
    "    \"\"\"\n",
    "    df_nan is output of nan_checker(..) function which returns df that contains columns: var, proportion, dtype\n",
    "    \"\"\"\n",
    "    \n",
    "    # we want to impute columns which have data type of float64 and keep columns of data type string\n",
    "    df_miss = df_nan[df_nan['dtype'] == 'float64'].reset_index(drop=True)\n",
    "    \n",
    "    # If there are missing values\n",
    "    if len(df_miss['var']) > 0:\n",
    "        # The SimpleImputer\n",
    "        si = SimpleImputer(missing_values=np.nan, strategy=strategy)\n",
    "\n",
    "        # Impute the variables with missing values in df_train, df_val and df_test \n",
    "        df_train[df_miss['var']] = si.fit_transform(df_train[df_miss['var']])\n",
    "        df_val[df_miss['var']] = si.transform(df_val[df_miss['var']])\n",
    "        df_test[df_miss['var']] = si.transform(df_test[df_miss['var']])\n",
    "    \n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_var_checker(df, dtype='object'):\n",
    "    \"\"\"\n",
    "    The categorical variable checker\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : the dataframe (train + test + validation)\n",
    "    dtype : the data type categorical variables should have, 'object' by default\n",
    "            i.e., if a variable has this data type, it should be a categorical variable\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    The dataframe of categorical variables and their number of unique value\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the dataframe of categorical variables and their number of unique value\n",
    "    df_cat = pd.DataFrame([[var, df[var].nunique(dropna=False)]\n",
    "                           # If the data type is dtype\n",
    "                           for var in df.columns if df[var].dtype == dtype],\n",
    "                          columns=['var', 'nunique'])\n",
    "    \n",
    "    # Sort df_cat in accending order of the number of unique value\n",
    "    df_cat = df_cat.sort_values(by='nunique', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return df_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_one_hot_encoding(df, df_cat, target):\n",
    "    \"\"\"\n",
    "    df_cat is obtained from cat_var_checker\n",
    "    df contains (train + test + validation)\n",
    "    \"\"\"\n",
    "    df = pd.get_dummies(df, columns=np.setdiff1d(df_cat['var'], [target]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_features_and_targets(df_train, df_val, df_test, target):\n",
    "    # Get the feature matrix\n",
    "    X_train = df_train[np.setdiff1d(df_train.columns, [target])].values\n",
    "    X_val = df_val[np.setdiff1d(df_val.columns, [target])].values\n",
    "    X_test = df_test[np.setdiff1d(df_test.columns, [target])].values\n",
    "\n",
    "    # Get the target vector\n",
    "    y_train = df_train[target].values\n",
    "    y_val = df_val[target].values\n",
    "    y_test = df_test[target].values\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Min-Max Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_min_max_scaler(X_train, y_train, X_val, y_val, X_test, y_test,  classification_problem=True):\n",
    "    \"\"\"\n",
    "    - all np arrays formed using split_features_and_targets(...)\n",
    "    - classification_problem=True, means the target column will not be normalized\n",
    "    - classification_problem=False, means the target column will be normalized\n",
    "    \"\"\"\n",
    "    mms = MinMaxScaler()\n",
    "    \n",
    "    # normalize the feature data\n",
    "    # find statistics based on training data\n",
    "    X_mms_train = mms.fit_transform(X_train)\n",
    "\n",
    "    # Normalize the validation data\n",
    "    X_mms_val = mms.transform(X_val)\n",
    "\n",
    "    # Normalize the test data\n",
    "    X_mms_test = mms.transform(X_test)\n",
    "    \n",
    "    # perform normalization on target variable in regression problem\n",
    "    if not classification_problem:\n",
    "        # Normalize the training data\n",
    "        y_mms_train = mms.fit_transform(y_train.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "        # Normalize the validation data\n",
    "        y_mms_val = mms.transform(y_val.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "        # Normalize the test data\n",
    "        y_mms_test = mms.transform(y_test.reshape(-1, 1)).reshape(-1)\n",
    "    \n",
    "    return X_mms_train, y_mms_train, X_mms_val, y_mms_val, X_mms_test, y_mms_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardization Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_standard_scaler(X_train, y_train, X_val, y_val, X_test, y_test,  classification_problem=True):\n",
    "    \"\"\"\n",
    "    - all np arrays formed using split_features_and_targets(...)\n",
    "    - classification_problem=True, means the target column will not be normalized\n",
    "    - classification_problem=False, means the target column will be normalized\n",
    "    \"\"\"\n",
    "    # The StandardScaler\n",
    "    ss = StandardScaler()\n",
    "    \n",
    "    # normalize the feature data\n",
    "    # find statistics based on training data\n",
    "    # Standardize the training data\n",
    "    X_ss_train = ss.fit_transform(X_train)\n",
    "\n",
    "    # Standardize the validation data\n",
    "    X_ss_val = ss.transform(X_val)\n",
    "\n",
    "    # Standardize the test data\n",
    "    X_ss_test = ss.transform(X_test)\n",
    "    \n",
    "    # perform normalization on target variable in regression problem\n",
    "    if not classification_problem:\n",
    "        # Standardize the training data\n",
    "        y_ss_train = ss.fit_transform(y_train.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "        # Standardize the validation data\n",
    "        y_ss_val = ss.transform(y_val.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "        # Standardize the test data\n",
    "        y_ss_test = ss.transform(y_test.reshape(-1, 1)).reshape(-1)\n",
    "    \n",
    "    return X_ss_train, y_ss_train, X_ss_val, y_ss_val, X_ss_test, y_ss_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversampler(X_train, y_train, random_seed=42):\n",
    "    ros = RandomOverSampler(random_state=random_seed)\n",
    "    X_ros_train, y_ros_train = ros.fit_resample(X_train, y_train)\n",
    "    \n",
    "    return X_ros_train, y_ros_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote(X_train, y_train, random_seed=42):\n",
    "    smote = SMOTE(random_state=random_seed)\n",
    "\n",
    "    # Augment the training data\n",
    "    X_smote_train, y_smote_train = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    return X_smote_train, y_smote_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
