{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import os\n",
    "from sklearn.metrics import make_scorer\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# external imports\n",
    "%run ../common/utilities.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Pickled Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = load_object(\"../input_data/\", \"X_train\")\n",
    "y_train = load_object(\"../input_data/\", \"y_train\")\n",
    "X_val = load_object(\"../input_data/\", \"X_val\")\n",
    "y_val = load_object(\"../input_data/\", \"y_val\")\n",
    "X_test = load_object(\"../input_data/\", \"X_test\")\n",
    "y_test = load_object(\"../input_data/\", \"y_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    97.92176\n",
       "1     2.07824\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y_val).value_counts()/len(y_val)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    97.861943\n",
       "1     2.138057\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y_test).value_counts()/len(y_test)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_smote, y_smote = perform_smote(X_train, y_train, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(X_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4908"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5044"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4908"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5044"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Predefined Split Cross Validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, y_train_val, ps_original = get_train_val_ps(X_smote, y_smote, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directory\n",
    "directory = os.path.dirname('./metric_logs/')\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'random_forest': RandomForestClassifier(class_weight='balanced', random_state=42)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipes = {}\n",
    "\n",
    "for acronym, model in models.items():\n",
    "    pipes[acronym] = Pipeline([('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {}\n",
    "\n",
    "# The grids for min_samples_split\n",
    "min_samples_split_grids = [2,5, 10, 15, 20, 30, 50, 80, 100]\n",
    "\n",
    "# The grids for min_samples_leaf\n",
    "min_samples_leaf_grids = [1, 2, 5, 10, 15, 20, 30, 50, 80, 100]\n",
    "\n",
    "# Update param_grids\n",
    "param_grids['random_forest'] = [{'model__min_samples_split': min_samples_split_grids,\n",
    "                                 'model__min_samples_leaf': min_samples_leaf_grids}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute F2 score for Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_score</th>\n",
       "      <th>best_param</th>\n",
       "      <th>best_estimator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.416667</td>\n",
       "      <td>{'model__min_samples_leaf': 5, 'model__min_sam...</td>\n",
       "      <td>((DecisionTreeClassifier(max_features='auto', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   best_score                                         best_param  \\\n",
       "0    0.416667  {'model__min_samples_leaf': 5, 'model__min_sam...   \n",
       "\n",
       "                                      best_estimator  \n",
       "0  ((DecisionTreeClassifier(max_features='auto', ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The list of [best_score_, best_params_, best_estimator_] obtained by GridSearchCV\n",
    "best_score_params_estimator_gs = []\n",
    "\n",
    "# For each model\n",
    "for acronym in pipes.keys():\n",
    "    # GridSearchCV\n",
    "    gs = GridSearchCV(estimator=pipes[acronym],\n",
    "                      param_grid=param_grids[acronym],\n",
    "                      scoring=make_scorer(f2_measure),\n",
    "                      n_jobs=-1,\n",
    "                      cv=ps_original,\n",
    "                      return_train_score=True)\n",
    "        \n",
    "    # Fit the pipeline\n",
    "    gs = gs.fit(X_train_val, y_train_val)\n",
    "    \n",
    "    # Update best_score_params_estimator_gs\n",
    "    best_score_params_estimator_gs.append([gs.best_score_, gs.best_params_, gs.best_estimator_])\n",
    "    \n",
    "    # Sort cv_results in ascending order of 'rank_test_score' and 'std_test_score'\n",
    "    cv_results = pd.DataFrame.from_dict(gs.cv_results_).sort_values(by=['rank_test_score', 'std_test_score'])\n",
    "    \n",
    "    # Get the important columns in cv_results\n",
    "    important_columns = ['rank_test_score',\n",
    "                         'mean_test_score', \n",
    "                         'std_test_score', \n",
    "                         'mean_train_score', \n",
    "                         'std_train_score',\n",
    "                         'mean_fit_time', \n",
    "                         'std_fit_time',                        \n",
    "                         'mean_score_time', \n",
    "                         'std_score_time']\n",
    "    \n",
    "    # Move the important columns ahead\n",
    "    cv_results = cv_results[important_columns + sorted(list(set(cv_results.columns) - set(important_columns)))]\n",
    "\n",
    "    # Write cv_results file\n",
    "    cv_results.to_csv(path_or_buf=directory +\"/\"+ acronym + '.csv', index=False)\n",
    "\n",
    "# Sort best_score_params_estimator_gs in descending order of the best_score_\n",
    "best_score_params_estimator_gs = sorted(best_score_params_estimator_gs, key=lambda x : x[0], reverse=True)\n",
    "\n",
    "# Print best_score_params_estimator_gs\n",
    "pd.DataFrame(best_score_params_estimator_gs, columns=['best_score', 'best_param', 'best_estimator'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute F2 score for Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best_score, best_params and best_estimator obtained by GridSearchCV\n",
    "best_score_gs, best_params_gs, best_estimator_gs = best_score_params_estimator_gs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F2 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.407801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   F2 Score\n",
       "0  0.407801"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the prediction on the testing data using best_model\n",
    "y_test_pred = best_estimator_gs.predict(X_test)\n",
    "\n",
    "# Get the f2 score\n",
    "f2_score = f2_measure(y_test, y_test_pred)\n",
    "\n",
    "pd.DataFrame([{\"F2 Score\":f2_score}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best model\n",
    "save_object(\"./best_saved_models/\", \"random_forest\", best_estimator_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
